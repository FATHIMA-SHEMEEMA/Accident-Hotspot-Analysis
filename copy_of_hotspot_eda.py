# -*- coding: utf-8 -*-
"""Copy of Hotspot_EDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_L46rzo4mhu4dqGhAtytwWTMu3zj1w43
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
import folium

data= pd.read_csv(r'C:\Users\FATHIMA SHEMEEMA\Music\internship\App_Hotspot\combined_data.csv')
# data.head()

# #show all the columns
# pd.set_option('display.max_columns', None)
# data.head(3)

# data.shape

# data.info()

# data.columns

# data.describe()

# data.isna().sum()

#drop T-junction
# data= data.drop('T -Junction', axis=1)

# data.columns

# data.shape

# data['Accident type'].value_counts()

# data['Type Area'].value_counts()

# data['City/Town/ Village'].value_counts()

# data['Lanes Road'].value_counts()

# data['Divider'].value_counts()

# data['Spot Accident'].value_counts()

# data['Weather'].value_counts()

# data['Collision'].value_counts()

# data['Type Road'].value_counts()

# data['Road Features'].value_counts()

# data['Visibility'].value_counts()

# data['Traffic Control'].value_counts()

# histplot=data.hist(figsize=(10,10))

#Extract numerical columns
numerical_columns= data.select_dtypes(include=['number']).columns
# numerical_columns

# import matplotlib.pyplot as plt
# import seaborn as sns
#checking outliers
# for col in numerical_columns:
#   plt.figure(figsize=(5,5))
#   sns.boxplot(data[col])
#   plt.show()

# data['Death'].value_counts()

# data['Grievous'].value_counts()

# data['Minor'].value_counts()

# data['Pedestrian'].value_counts()



# data.columns

# Group by relevant features to get accident counts and include Latitude and Longitude
accident_counts = data.groupby(['District', 'Spot Accident','PS Name']).size().reset_index(name='Accident_Count')



# accident_counts.head(10)

# accident_counts['Accident_Count'].unique()

# accident_counts['Accident_Count'].value_counts()

# Manually assign clusters based on your criteria with descriptive labels
def assign_manual_cluster(accident_count):
    if accident_count <= 5:
        return 1, "Low incident area"

    else:
        return 2, "Hotspot"

# Apply the modified function to create clusters and labels
accident_counts[['Cluster', 'Label']] = accident_counts['Accident_Count'].apply(assign_manual_cluster).apply(pd.Series)
#accident_counts[['Cluster', 'Label']]



"""

```
# This is formatted as code
```

Low Incident  area"""

accident_counts[accident_counts['Label'] == "not prone to aacidents"].head(5)



"""

```
# This is formatted as code
```
Hotspots"""

accident_counts[accident_counts['Label'] == "Accident prone area"].head(5)



#bargraph of cluster column
# accident_counts['Cluster'].value_counts().plot(kind='bar')

# accident_counts['Cluster'].value_counts()

# accident_counts.shape

# accident_counts.head()



"""Encoding"""



# Perform encoding on categorical features
freq_encoded = ['District','Spot Accident', 'PS Name']

# Apply Frequency Encoding
for col in freq_encoded:
    # Compute frequency of each category in the column
    frequency_map = accident_counts[col].value_counts(normalize=True)

    # Map these frequencies to the original column
    accident_counts[col] = accident_counts[col].map(frequency_map)

# Display the DataFrame to see the encoded columns
#accident_counts.head()

# Prepare features (X) and labels (y)
X = accident_counts.drop(['Accident_Count', 'Cluster', 'Label'], axis=1)
y = accident_counts['Cluster']

#X.shape

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize the Random Forest Classifier
#model = RandomForestClassifier(random_state=42)
model = RandomForestClassifier(class_weight='balanced', random_state=42)
#model = RandomForestClassifier(n_estimators=50, max_depth=5, random_state=42)
#model = RandomForestClassifier(min_samples_leaf=10, min_samples_split=10, random_state=42)

# Train the model
model.fit(X_train, y_train)

# Evaluate the model
y_pred = model.predict(X_test)

from sklearn.metrics import accuracy_score

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy * 100:.2f}%')

# Print evaluation metrics
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

## Function to predict the cluster for new input values including Latitude and Longitude
def predict_cluster(district, ps_name,accident_spot):
    new_input = pd.DataFrame({
        'District': [district],
        'PS Name': [ps_name],
        'Spot Accident': [accident_spot]


    })

    # One-hot encode the new input
    new_input_encoded = pd.get_dummies(new_input, columns=['District', 'PS Name', 'Spot Accident'], drop_first=True)

    # Align the new input with the training data columns
    new_input_encoded = new_input_encoded.reindex(columns=X.columns, fill_value=0)

    predicted_cluster = model.predict(new_input_encoded)
    return predicted_cluster[0]  # Return the predicted cluster

# Get the prediction for the new input values
district = 'ALAPPUZHA'  # Replace with actual district name
ps_name="Mavelikara	"
accident_spot= 'Affected by encroachments'


predicted_cluster = predict_cluster(district,ps_name, accident_spot)

# Display the result
print(f'Predicted Cluster: {predicted_cluster}')
#print(f'Prediction Probability: {prediction_probability:.2f}%')










#add latitude and longitude columns from data variable in the account_counts variable
accident_counts['Latitude'] = data['Latitude']
accident_counts['Longitude'] = data['Longitude']
#accident_counts.head()

"""#Map Visualization"""

# Create a map to visualize accident hotspots
# Initialize a folium map centered around the mean latitude and longitude
map_center = [accident_counts['Latitude'].mean(), accident_counts['Longitude'].mean()]
accident_map = folium.Map(location=map_center, zoom_start=12)

# Add markers for each accident hotspot
for idx, row in accident_counts.iterrows():
    folium.CircleMarker(
        location=(row['Latitude'], row['Longitude']),
        radius=5,  # Circle radius
        color='blue' if row['Cluster'] == 1 else 'red',
        fill=True,
        fill_color='blue' if row['Cluster'] == 1 else 'red' ,
        fill_opacity=0.6,
        popup=f"Accident Count: {row['Accident_Count']}<br>Cluster: {row['Label']}"
    ).add_to(accident_map)

# Save the map to an HTML file
accident_map.save('accident_hotspot_map.html')

# Display the map in a Jupyter Notebook or similar environment
accident_map





#pickle

# import pickle
# filename = 'model.pkl'
# pickle.dump(model, open(filename, 'wb'))

# # import pandas as pd

# # # Assuming accident_counts is your DataFrame after frequency encoding

# # Save the DataFrame as a pickle file
# pickle_file_path = 'accident_counts_encoded.pkl'
# accident_counts.to_pickle(pickle_file_path)

# print(f"DataFrame saved as pickle file: {pickle_file_path}")



# Load the DataFrame from the pickle file

# loaded_accident_counts = pd.read_pickle(pickle_file_path)
# print(loaded_accident_counts.head())